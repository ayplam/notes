{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: https://github.com/siddharth-agrawal/Softmax-Regression/blob/master/softmaxRegression.py\n",
    "### Theory: http://cs229.stanford.edu/notes/cs229-notes1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Logistic Regression\n",
    "\n",
    "A basic linear regression usually has binary outputs/classifications (ie, has cancer, does not have cancer). What if you had more than two classifications, like the MNIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original linear regression, your hypothesis function to guess what the probability of your output was represented by:\n",
    "$$ h_\\theta(x) = \\frac{1}{(1 + e^{-\\theta^Tx})} $$\n",
    "\n",
    "where the cost function was:\n",
    "$$ J(\\theta) = -\\Bigg[\n",
    "\\sum_{i=1}^{m}\n",
    "y^{(i)}log(h_\\theta(x^{(i)})) + (1 - y^{(i)}log(1-h_\\theta(x^{(i)}))\n",
    "\\Bigg] $$\n",
    "\n",
    "In the softmax regression, you want to do multiple classifications, like in MNIST, which has K = 10 classes. Thus, given a test input x, you want the hypothesis to estimate the probability that **P(y=k|x)** for each class k. This means that the output of the softmax is actually a K x 1 vector, where each value in the vector is the probability that x is in class K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy\n",
    "import array\n",
    "import time\n",
    "import scipy.sparse\n",
    "import scipy.optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "\"\"\" Loads the images from the provided file name \"\"\"\n",
    "\n",
    "def loadMNISTImages(file_name):\n",
    "\n",
    "    \"\"\" Open the file \"\"\"\n",
    "\n",
    "    image_file = open(file_name, 'rb')\n",
    "    \n",
    "    \"\"\" Read header information from the file \"\"\"\n",
    "    \n",
    "    head1 = image_file.read(4)\n",
    "    head2 = image_file.read(4)\n",
    "    head3 = image_file.read(4)\n",
    "    head4 = image_file.read(4)\n",
    "    \n",
    "    \"\"\" Format the header information for useful data \"\"\"\n",
    "    \n",
    "    num_examples = struct.unpack('>I', head2)[0]\n",
    "    num_rows     = struct.unpack('>I', head3)[0]\n",
    "    num_cols     = struct.unpack('>I', head4)[0]\n",
    "    \n",
    "    \"\"\" Initialize dataset as array of zeros \"\"\"\n",
    "    \n",
    "    dataset = numpy.zeros((num_rows*num_cols, num_examples))\n",
    "    \n",
    "    \"\"\" Read the actual image data \"\"\"\n",
    "    \n",
    "    images_raw  = array.array('B', image_file.read())\n",
    "    image_file.close()\n",
    "    \n",
    "    \"\"\" Arrange the data in columns \"\"\"\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "    \n",
    "        limit1 = num_rows * num_cols * i\n",
    "        limit2 = num_rows * num_cols * (i + 1)\n",
    "        \n",
    "        dataset[:, i] = images_raw[limit1 : limit2]\n",
    "    \n",
    "    \"\"\" Normalize and return the dataset \"\"\"    \n",
    "            \n",
    "    return dataset / 255\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Loads the image labels from the provided file name \"\"\"\n",
    "    \n",
    "def loadMNISTLabels(file_name):\n",
    "\n",
    "    \"\"\" Open the file \"\"\"\n",
    "\n",
    "    label_file = open(file_name, 'rb')\n",
    "    \n",
    "    \"\"\" Read header information from the file \"\"\"\n",
    "    \n",
    "    head1 = label_file.read(4)\n",
    "    head2 = label_file.read(4)\n",
    "    \n",
    "    \"\"\" Format the header information for useful data \"\"\"\n",
    "    \n",
    "    num_examples = struct.unpack('>I', head2)[0]\n",
    "    \n",
    "    \"\"\" Initialize data labels as array of zeros \"\"\"\n",
    "    \n",
    "    labels = numpy.zeros((num_examples, 1), dtype = numpy.int)\n",
    "    \n",
    "    \"\"\" Read the label data \"\"\"\n",
    "    \n",
    "    labels_raw = array.array('b', label_file.read())\n",
    "    label_file.close()\n",
    "    \n",
    "    \"\"\" Copy and return the label data \"\"\"\n",
    "    \n",
    "    labels[:, 0] = labels_raw[:]\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Loads data, trains the model and predicts classes for test data '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################################################################################\n",
    "\"\"\" The Softmax Regression class \"\"\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Initialization of Regressor object \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, num_classes, lamda):\n",
    "    \n",
    "        \"\"\" Initialize parameters of the Regressor object \"\"\"\n",
    "    \n",
    "        self.input_size  = input_size  # input vector size\n",
    "        self.num_classes = num_classes # number of classes\n",
    "        self.lamda       = lamda       # weight decay parameter\n",
    "        \n",
    "        \"\"\" Randomly initialize the class weights \"\"\"\n",
    "        \n",
    "        rand = numpy.random.RandomState(int(time.time()))\n",
    "        \n",
    "        self.theta = 0.005 * numpy.asarray(rand.normal(size = (num_classes*input_size, 1)))\n",
    "    \n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the groundtruth matrix for a set of labels \"\"\"\n",
    "    \n",
    "    # The ground truth matrix is actually the \"vector\" notation of the output.\n",
    "    # So the label looks like [4,3,7,1]\n",
    "    # The groundTruth matrix will look like:\n",
    "    # [[0 0 0 0]      row = 0\n",
    "    #  [0 0 0 1]      row = 1\n",
    "    #  [0 0 0 0]      row = 2\n",
    "    #  [0 1 0 0]      etc....\n",
    "    #  [1 0 0 0]\n",
    "    #  [0 0 0 0]\n",
    "    #  [0 0 0 0] \n",
    "    #  [0 0 1 0]      row = 7\n",
    "    #  [0 0 0 0]\n",
    "    #  [0 0 0 0]]\n",
    "    # where each columns is the \"ground truth\" vector output    \n",
    "        \n",
    "    def getGroundTruth(self, labels):\n",
    "    \n",
    "        \"\"\" Prepare data needed to construct groundtruth matrix \"\"\"\n",
    "    \n",
    "        labels = numpy.array(labels).flatten()\n",
    "        data   = numpy.ones(len(labels))\n",
    "        indptr = numpy.arange(len(labels)+1)\n",
    "        \n",
    "        \"\"\" Compute the groundtruth matrix and return \"\"\"\n",
    "        \n",
    "        ground_truth = scipy.sparse.csr_matrix((data, labels, indptr))\n",
    "        ground_truth = numpy.transpose(ground_truth.todense())\n",
    "        \n",
    "        return ground_truth\n",
    "        \n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the cost and gradient of 'theta' at a particular 'theta' \"\"\"\n",
    "        \n",
    "    def softmaxCost(self, theta, input, labels):\n",
    "    \n",
    "        \"\"\" Compute the groundtruth matrix \"\"\"\n",
    "    \n",
    "        ground_truth = self.getGroundTruth(labels)\n",
    "        \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "        \n",
    "        # 10 x 784, so the same shape as the ground_truth matrix. Should make sense \n",
    "        # since each one needs a weight.\n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        print \"ThetaShape: \", theta.shape\n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        # Matrix multiplication of theta X input. Input is 784 x 600k, so each \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        print \"Probabilities: \",  probabilities.shape\n",
    "        \"\"\" Compute the traditional cost term \"\"\"\n",
    "        \n",
    "        cost_examples    = numpy.multiply(ground_truth, numpy.log(probabilities))\n",
    "        print \"CostExamples: \", cost_examples.shape\n",
    "        traditional_cost = -(numpy.sum(cost_examples) / input.shape[1])\n",
    "        print \"TraditionalCost: \", traditional_cost.shape\n",
    "        \n",
    "        \"\"\" Compute the weight decay term \"\"\"\n",
    "        \n",
    "        theta_squared = numpy.multiply(theta, theta)\n",
    "        weight_decay  = 0.5 * self.lamda * numpy.sum(theta_squared)\n",
    "        \n",
    "        \"\"\" Add both terms to get the cost \"\"\"\n",
    "        \n",
    "        cost = traditional_cost + weight_decay\n",
    "        \n",
    "        \"\"\" Compute and unroll 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = -numpy.dot(ground_truth - probabilities, numpy.transpose(input))\n",
    "        theta_grad = theta_grad / input.shape[1] + self.lamda * theta\n",
    "        theta_grad = numpy.array(theta_grad)\n",
    "        theta_grad = theta_grad.flatten()\n",
    "        \n",
    "        return [cost, theta_grad]\n",
    "    \n",
    "    #######################################################################################\n",
    "    \"\"\" Returns predicted classes for a set of inputs \"\"\"\n",
    "            \n",
    "    def softmaxPredict(self, theta, input):\n",
    "    \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "    \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        \n",
    "        \"\"\" Give the predictions based on probability values \"\"\"\n",
    "        \n",
    "        predictions = numpy.zeros((input.shape[1], 1))\n",
    "        predictions[:, 0] = numpy.argmax(probabilities, axis = 0)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Loads data, trains the model and predicts classes for test data \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Initialize parameters of the Regressor \"\"\"\n",
    "\n",
    "input_size     = 784    # input vector size\n",
    "num_classes    = 10     # number of classes\n",
    "lamda          = 0.0001 # weight decay parameter\n",
    "max_iterations = 100    # number of optimization iterations\n",
    "\n",
    "\"\"\" Load MNIST training images and labels \"\"\"\n",
    "\n",
    "training_data   = loadMNISTImages('train-images.idx3-ubyte')\n",
    "training_labels = loadMNISTLabels('train-labels.idx1-ubyte')\n",
    "\n",
    "\"\"\" Initialize Softmax Regressor with the above parameters \"\"\"\n",
    "\n",
    "regressor = SoftmaxRegression(input_size, num_classes, lamda)\n",
    "\n",
    "\"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "\n",
    "opt_solution  = scipy.optimize.minimize(regressor.softmaxCost, regressor.theta, \n",
    "                                        args = (training_data, training_labels,), method = 'L-BFGS-B', \n",
    "                                        jac = True, options = {'maxiter': max_iterations})\n",
    "opt_theta     = opt_solution.x\n",
    "\n",
    "\"\"\" Load MNIST test images and labels \"\"\"\n",
    "\n",
    "test_data   = loadMNISTImages('t10k-images.idx3-ubyte') \n",
    "test_labels = loadMNISTLabels('t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Initialize parameters of the Regressor \"\"\"\n",
    "\n",
    "input_size     = 784    # input vector size\n",
    "num_classes    = 10     # number of classes\n",
    "lamda          = 0.0001 # weight decay parameter\n",
    "max_iterations = 100    # number of optimization iterations\n",
    "\n",
    "\"\"\" Load MNIST training images and labels \"\"\"\n",
    "\n",
    "training_data   = loadMNISTImages('train-images.idx3-ubyte')\n",
    "training_labels = loadMNISTLabels('train-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7840L, 1L)\n",
      "(784L, 60000L)\n"
     ]
    }
   ],
   "source": [
    "print regressor.theta.shape\n",
    "print training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThetaShape:  (10L, 784L)\n",
      "Probabilities:  (10L, 60000L)\n",
      "CostExamples:  (10L, 60000L)\n",
      "TraditionalCost:  ()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.306906847339051,\n",
       " array([ -3.34413173e-07,  -5.83975986e-07,   4.99063465e-08, ...,\n",
       "         -3.83330175e-09,   8.16554519e-07,  -7.81587172e-08])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Initialize Softmax Regressor with the above parameters \"\"\"\n",
    "\n",
    "regressor = SoftmaxRegression(input_size, num_classes, lamda)\n",
    "regressor.softmaxCost(regressor.theta,training_data,training_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9259\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "\n",
    "opt_solution  = scipy.optimize.minimize(regressor.softmaxCost, regressor.theta, \n",
    "                                        args = (training_data, training_labels,), method = 'L-BFGS-B', \n",
    "                                        jac = True, options = {'maxiter': max_iterations})\n",
    "opt_theta     = opt_solution.x\n",
    "\n",
    "\"\"\" Load MNIST test images and labels \"\"\"\n",
    "\n",
    "test_data   = loadMNISTImages('t10k-images.idx3-ubyte') \n",
    "test_labels = loadMNISTLabels('t10k-labels.idx1-ubyte')\n",
    "\n",
    "\"\"\" Obtain predictions from the trained model \"\"\"\n",
    "\n",
    "predictions = regressor.softmaxPredict(opt_theta, test_data)\n",
    "\n",
    "correct = test_labels[:, 0] == predictions[:, 0]\n",
    "print \"\"\"Accuracy :\"\"\", numpy.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10L, 60000L)\n",
      "(60000L, 1L)\n"
     ]
    }
   ],
   "source": [
    "print regressor.getGroundTruth(training_labels).shape\n",
    "print training_labels.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
